\section{Monte Carlo Non-Local Means}

In this section, we discuss the full implementation of the algorithm, the correctness of the approach and some other optimizations that can be added to the method.

\subsection{Sampling Patches}

There are two ways to extract reference patches. The first one is based on picking them from the original noisy, image, which is called \textit{internal denoising}. The second method is based on having the patches taken from a large database of other images, which is called \textit{external denoising}. The paper focuses on internal denoising.

The Monte Carlo sampling is done on the set $\mathcal{X} = \{x_1, \dots, x_n \}$, considering each reference patch independent. The process is determined by a sequence of random variables $\{I_j\}_{j=1}^n$, where $I_j \sim \operatorname{Bernoulli}(p_j)$. The weight will be sampled only if $I_j = 1$. The vector of these probabilities, $\mathbf{p} := [p_1, \dots, p_n]^T$, is called the \textit{sampling pattern} of the algorithm \cite{mcnlm}.

The main parameter of this algorithm is $\xi$, which is the expected value of the random variable which models the ratio between the number of the samples taken and the number of references:
\begin{equation}
    S_n = \frac{1}{n} \sum_{j = 1}^n I_j
\end{equation}
\noindent which has:
\begin{equation}
    \xi \overset{\text{def}}{=} \mathbb{E}[S_n] = \frac{1}{n} \sum_{j = 1}^{n} \mathbb{E}[I_j] = \frac{1}{n}\sum_{j = 1}^{n} p_j
\end{equation}

So, an important requirement is that:
\begin{equation}
    \sum_{j=1}^{n} = n \xi
\end{equation}

$S_n$ is called the \textit{empirical sampling ratio} and $\xi$ the average sampling ratio \cite{mcnlm}.

\subsection{Algorithm}
\label{sec:algo}
Given a set of references $\mathcal{X}$ and the sampling ration $\xi$, we can define the sampling pattern $\mathbf{p}$ in order for it to respect the condition:
\begin{equation}
    \sum_{j = 1}^{n} p_j  = n \xi
\end{equation}.

We can approximate the numerator and the denominator in \ref{eq:nlm} using two random variables:
\begin{equation}
    A = \frac{1}{n} \sum_{j = 1}^{n} x_j w_j \frac{I_j}{p_j} \quad \text{  and  } \quad
    B = \frac{1}{n} \sum_{j = 1}^{n} w_j \frac{I_j}{p_j}
\end{equation}

To show why we scale by $\frac{1}{p_j}$, we calculate the expected value of the estimators. By linearity of expectation:
\begin{equation}
    \mathbb{E}[A] = \frac{1}{n} \sum_{j=1}^{n} x_j w_j \frac{\mathbb{E}[I_j]}{p_j}
    = \frac{1}{n} \sum_{j=1}^{n} x_j w_j \frac{p_j}{p_j} =  \frac{1}{n} \sum_{j=1}^{n} x_j w_j
\end{equation}
\noindent and
\begin{equation}
    \mathbb{E}[B] = \frac{1}{n} \sum_{j = 1}^{n} w_j \frac{\mathbb{E}[I_j]}{p_j} = \frac{1}{n} \sum_{j = 1}^n w_j
\end{equation}

So, A and B are \textit{unbiased estimators} of the true numerator and denominator.

In the end, we will aproximate the result $z$ by another random variable:
\begin{equation}
    Z = \frac{A}{B} = \frac{\sum_{j=1}^{n} x_j w_j \frac{I_j}{p_j}}{\sum_{j = 1}^{n} w_j\frac{I_j}{p_j}}
\end{equation}

However,
\begin{equation}
    \mathbb{E}[Z] = \mathbb{E}\left[\frac{A}{B}\right] \neq \frac{\mathbb{E}[A]}{\mathbb{E}[B]} = z
\end{equation}

% todo - implement section label sec:ineq when I atually implement the inequality
\noindent so, $Z$ is a \textit{biased estimator} of $z$. However, section \ref{sec:ineq} proves that as $n \to \infty$, the deviation $|Z - z|$ drops exponentially. This shows that, for a larger number of patches, the more efficient MCNLM gives results comparable to that of the simple NLM.

\FloatBarrier
\begin{algorithm}[H]
\caption{Monte Carlo NLM Denoising}
\label{alg:mcnlm}
\begin{algorithmic}[1]
\Require Noisy patch $\mathbf{y}$, reference set $\mathcal{X}$, sampling pattern $\mathbf{p}$
\Ensure Denoised pixel $z$
\For{$j$ in $1 \dots n$}
    \State Generate random variable $I_j \sim \operatorname{Bernoulli}(p_j)$
    \If{$I_j = 1$}
        \State Compute $w_j$
    \EndIf
\EndFor
\State Compute $A = \frac{1}{n} \sum_{j = 1}^{n} w_j x_j \frac{I_j}{p_j}$
\State Compute $B = \frac{1}{n} \sum_{j = 1}^{n} w_j  \frac{I_j}{p_j}$
\State \Return $Z = \frac{A}{B}$
\end{algorithmic}
\end{algorithm}


\subsection{Results}

% aici vreau sa avem results cu diferite valori xi, comparate