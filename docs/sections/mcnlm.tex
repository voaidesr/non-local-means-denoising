\section{Monte Carlo Non-Local Means}

In this section, we discuss the full implementation of the algorithm, the correctness of the approach and some other optimizations that can be added to the method.

\subsection{Sampling Patches}

There are two ways to extract reference patches. The first one is based on picking them from the original noisy, image, which is called \textit{internal denoising}. The second method is based on having the patches taken from a large database of other images, which is called \textit{external denoising}. The paper focuses on internal denoising.

The Monte Carlo sampling is done on the set $\mathcal{X} = \{x_1, \dots, x_n \}$, considering each reference patch independent. The process is determined by a sequence of random variables $\{I_j\}_{j=1}^n$, where $I_j \sim \operatorname{Bernoulli}(p_j)$. The weight will be sampled only if $I_j = 1$. The vector of these probabilities, $\mathbf{p} := [p_1, \dots, p_n]^T$, is called the \textit{sampling pattern} of the algorithm \cite{mcnlm}.

The main parameter of this algorithm is $\xi$, which is the expected value of the random variable which models the ratio between the number of the samples taken and the number of references:
\begin{equation}
    S_n = \frac{1}{n} \sum_{j = 1}^n I_j
\end{equation}
\noindent which has:
\begin{equation}
    \xi \overset{\text{def}}{=} \mathbb{E}[S_n] = \frac{1}{n} \sum_{j = 1}^{n} \mathbb{E}[I_j] = \frac{1}{n}\sum_{j = 1}^{n} p_j
\end{equation}

So, an important requirement is that:
\begin{equation}
    \sum_{j=1}^{n} = n \xi
\end{equation}

$S_n$ is called the \textit{empirical sampling ratio} and $\xi$ the average sampling ratio \cite{mcnlm}.

\subsection{Algorithm}
\label{sec:algo}
Given a set of references $\mathcal{X}$ and the sampling ration $\xi$, we can define the sampling pattern $\mathbf{p}$ in order for it to respect the condition:
\begin{equation}
    \sum_{j = 1}^{n} p_j  = n \xi
\end{equation}.

We can approximate the numerator and the denominator in \ref{eq:nlm} using two random variables:
\begin{equation}
    A = \frac{1}{n} \sum_{j = 1}^{n} x_j w_j \frac{I_j}{p_j} \quad \text{  and  } \quad
    B = \frac{1}{n} \sum_{j = 1}^{n} w_j \frac{I_j}{p_j}
\end{equation}

To show why we scale by $\frac{1}{p_j}$, we calculate the expected value of the estimators. By linearity of expectation:
\begin{equation}
    \mathbb{E}[A] = \frac{1}{n} \sum_{j=1}^{n} x_j w_j \frac{\mathbb{E}[I_j]}{p_j}
    = \frac{1}{n} \sum_{j=1}^{n} x_j w_j \frac{p_j}{p_j} =  \frac{1}{n} \sum_{j=1}^{n} x_j w_j
\end{equation}
\noindent and
\begin{equation}
    \mathbb{E}[B] = \frac{1}{n} \sum_{j = 1}^{n} w_j \frac{\mathbb{E}[I_j]}{p_j} = \frac{1}{n} \sum_{j = 1}^n w_j
\end{equation}

So, A and B are \textit{unbiased estimators} of the true numerator and denominator.

In the end, we will aproximate the result $z$ by another random variable:
\begin{equation}
    Z = \frac{A}{B} = \frac{\sum_{j=1}^{n} x_j w_j \frac{I_j}{p_j}}{\sum_{j = 1}^{n} w_j\frac{I_j}{p_j}}
\end{equation}

However,
\begin{equation}
\label{eq:Z}
    \mathbb{E}[Z] = \mathbb{E}\left[\frac{A}{B}\right] \neq \frac{\mathbb{E}[A]}{\mathbb{E}[B]} = z
\end{equation}

% todo - implement section label sec:ineq when I atually implement the inequality
\noindent so, $Z$ is a \textit{biased estimator} of $z$. However, section \ref{sec:ineq} proves that as $n \to \infty$, the deviation $|Z - z|$ drops exponentially. This shows that, for a larger number of patches, the more efficient MCNLM gives results comparable to that of the simple NLM.

\FloatBarrier
\begin{algorithm}[H]
\caption{Monte Carlo NLM Denoising}
\label{alg:mcnlm}
\begin{algorithmic}[1]
\Require Noisy patch $\mathbf{y}$, reference set $\mathcal{X}$, sampling pattern $\mathbf{p}$
\Ensure Denoised pixel $z$
\For{$j$ in $1 \dots n$}
    \State Generate random variable $I_j \sim \operatorname{Bernoulli}(p_j)$
    \If{$I_j = 1$}
        \State Compute $w_j$
    \EndIf
\EndFor
\State Compute $A = \frac{1}{n} \sum_{j = 1}^{n} w_j x_j \frac{I_j}{p_j}$
\State Compute $B = \frac{1}{n} \sum_{j = 1}^{n} w_j  \frac{I_j}{p_j}$
\State \Return $Z = \frac{A}{B}$
\end{algorithmic}
\end{algorithm}


\subsection{Spatial Sampling}

We must also consider that locality matters in images. In a picture of a starry night, for example, a bright star might look like noise if compared to the rest of the dark sky. We can integrate this insight with the ingenious structural handling of MCNLM by incorporating a spatial distance parameter into the weighting function:
\begin{equation}
    w_j = w_j^s \cdot w_j^r
\end{equation}
\noindent where $w_j^r$ is the weight mentioned at \ref{eq:wr} and $w_j^s$ is a spatial weight:
\begin{equation}
w_j^s = e^{-(d_2^j)^2/(2h_s^2)} \cdot \mathbb{I}\{d^j_{\infty} \leq \rho\}
\end{equation}

\noindent where $d_j^2$ is the Euclidean distance between the noisy patch and the reference patch we compare to and $d_j^{\infty}$ is the Chebyshev distance. The  indicator function $\mathbb{I}$ is used to determine a spatial search window of width $\rho$.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{res/mc_matches_1.pdf}
        \caption{Weight values in search window}
        \label{fig:img1}
    \end{subfigure}
    \hfill % Adds flexible space between images
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{res/mc_matches_2.pdf}
        \caption{Weigths for sampled center pixels}
        \label{fig:img2}
    \end{subfigure}

    \caption{Main caption describing both images}
    \label{fig:both_images}
\end{figure}

Figure \ref{fig:both_images} visualizes the weight function within a search window centered on a noisy pixel, where brighter values indicate higher weight magnitudes. This illustrates how the method preserves structure: pixels with similar patches exert a stronger influence on the weighted average, ensuring that structural details are retained in the denoised output.

\subsection{Results}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{res/mcnlm2.pdf}
    \caption{MCNLM with $\xi \in \{0.3,  0.5, 0.8\}$}
    \label{fig1}
\end{figure}
% aici vreau sa avem results cu diferite valori xi, comparate

In the figures \ref{fig1}, \ref{fig2} we can see how the MCNLM algorithm behaves on an 1024x1024 image ($\approx 10^6$ pixels), with Gaussian noise with $\sigma = 17 / 255$ (taken into account that we normalize our pixels to have values in the interval $[0,1]$)/ and zero mean. The comparison is done on 5x5 patches, and on a 20x20 search window arround the noisy pixel. The sampling pattern is chosen as uniform, with $\mathbf{p} = \{\xi, \cdots, \xi\}$. It is obvious that an increase in the sampling ratio only provides a marginally better result.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{res/mcnlm3.pdf}
    \caption{MCNLM with $\xi \in \{0.3,  0.5, 0.8\}$}
    \label{fig2}
\end{figure}


\section{Stochastic Approximation of Non-Local Means}
We yield the question whether the Monte-Carlo variant of our algorithm is indeeed reliable and aproaches the full Non-Local Means solution. For this we need to analyze the approximation error $|Z - z|$.

To understand our approximation, we will study the empirical sampling ratio $S_{n}$. The law of large numbers states that the empirical mean $S_{n}$ of a large number of indepent random variable is close to the true mean.

\subsection{Probability Bounds}
For any error bound $\varepsilon > 0$ and any sampling pattern $p$ with $0 < p_{j} <= 1$ and $\sum_{j=1}^{n}p_{j} = \varepsilon$ we want to study the probability:
\begin{equation}
    \mathbb{P}(|S_{n} - \mathbb{E}[S_{n}]| > \varepsilon)
\end{equation}

From the Cebyshev inequality we know that:
\begin{equation}
    \mathbb{P}(|S_{n} - \mathbb{E}[S_{n}]| > \varepsilon) < \frac{Var[I_{1}]}{n\varepsilon^{2}}, \forall \varepsilon > 0
\end{equation}

This bound provided by the Cebyshev inequality above is too loose, only providing a linear descent of the error probability as $n \to \infty$.

We can use the Bernstein inequality to provide a much tighter exponential bound. The following is true for a sequence of $n$ Bernoulli i.i.d. variables:
\begin{equation}
    \mathbb{P}(|S_{n} - \mathbb{E}[S_{n}]| > \varepsilon) \le \operatorname{exp} \left (-\frac{n\varepsilon^2}{2\left(\xi(1 - \xi)+ \varepsilon/3\right)} \right )
\end{equation}

where $S_{n} = \frac{1}{n}\sum_{j=1}^{n}I_{j}$.


\subsection{General Error Probability Bound}
\label{sec:ineq}

To rigorously quantify the reliability of the Monte Carlo approximation, we refer to the concentration bound derived in \cite{mcnlm}. For a sample size $n$ and an error tolerance $\varepsilon > 0$, the probability that the Monte Carlo estimate $Z$ deviates from the exact NLM value $z$ is bounded by:

\begin{equation}
    \label{eq:full_bound}
    \begin{split}
    \mathbb{P}(|Z - z| > \varepsilon ) \leq & \exp \{-n\xi\} \\
    & + 2 \exp \left\{ \frac{-n(\mu_B \varepsilon)^2}{ 2 \left( \frac{1}{n} \sum_{j=1}^n \alpha_j^2 \left( \frac{1-p_j}{p_j} \right) + \frac{(\mu_B \varepsilon)M_\alpha}{6} \right) } \right\}
    \end{split}
\end{equation}

\subsubsection{Simplification for Uniform Sampling}
While Equation \ref{eq:full_bound} accounts for variable sampling probabilities, our implementation utilizes a uniform sampling pattern where $p_j = p$ for all patches. Furthermore we have $\mu_B \leq 1$.

Under these experimental conditions, the bound simplifies significantly. Neglecting the trivial failure term $\exp\{-n\xi\}$ (which approaches zero for our window size of $n=441$), the failure probability becomes:

\begin{equation}
    \label{eq:simple_bound}
    \mathbb{P}(|Z - z| > \varepsilon ) \leq 2 \exp \left( \frac{-n \varepsilon^2}{ 2 \left(\rho \sum_{j=1}^{n} \alpha_j^2 + \varepsilon/6 \right) } \right)
\end{equation}

\noindent where $n = 441$ is the total pixels in the $21 \times 21$ search window,
$\rho = (1-p)/p$ is the sampling penalty factor and
$\alpha_j = \frac{w_j}{\sum_{k = 1}^{n} w_k}$ is the normalized weight of the patch $j$.

\subsubsection{Derivation of Numerical Bounds}
To obtain a concrete probability bound, we will replace the values of the variables in Equation \ref{eq:simple_bound} with the parameters from our experimental setup. The parameters are, as follows, $n = 441$, for a $21 \times 21$ search window, $\rho = (1 - 0.5)/0.5 = 1$. We select the desired error tolerance of $\varepsilon = 0.05$.

The remaining term, $\sum_{j=1}^{n} \alpha_j^2$, is non-trivial as it depends on the image content. We can, however, derive bounds for two limiting cases:

\paragraph{Case 1: Homogeneous Regions}
In flat image regions (e.g., sky or smooth walls), the NLM weights are distributed nearly uniformly across the search window.
$$ w_j \approx w \quad \forall j \implies \alpha_j \approx \frac{1}{n} $$
Substituting this into the variance term:
$$ \sum_{j=1}^{n} \alpha_j^2 \approx \sum_{j=1}^{n} \left(\frac{1}{n}\right)^2 = n \cdot \frac{1}{n^2} = \frac{1}{441} \approx 0.0023 $$

From \ref{eq:simple_bound}, we calculate that the resulting probability is $\mathbb{P}(|Z - z| > \varepsilon) < 10^{-20}$, confirming that random sampling is statistically safe in smooth regions.

\paragraph{Case 2: Structured Regions}
In textured regions or near edges, the weights concentrate on a small subset of patches that have a high similarity to the patch we want to denoise. We estimate this by defining an \textit{effective neighbor count} $k_{\text{eff}}$. If the algorithm finds only $k_{\text{eff}} \approx 10$ strong matches in the window (a conservative estimate for detailed texture), the normalized weights approximate $\alpha_j \approx 1/10$ for the matches and $0$ otherwise.
$$ \sum_{j=1}^{n} \alpha_j^2 \approx \sum_{j=1}^{10} \left(\frac{1}{10}\right)^2 = 10 \cdot \frac{1}{100} = 0.1 $$
Using this estimate in  \ref{eq:simple_bound}:
$$ \text{Exponent} = \frac{-441 \cdot (0.05)^2}{2(1 \cdot 0.1 + 0.05/6)} \approx \frac{-1.1025}{0.216} \approx -5.1 $$

\noindent therefore
$$ \mathbb{P}(|Z - z| > \varepsilon) \leq 2 e^{-5.1} \approx 0.012 $$
This shows that even in worst-case scenarios, the probability of the Monte Carlo estimate deviating by more than $5\%$ is strictly bounded below $1.2\%$.


\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{res/convergence1_mse.pdf}
        \caption{MSE across different sampling probabilities}
        \label{fig:img1}
    \end{subfigure}
    \hfill % Adds flexible space between images
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{res/convergence1_psnr.pdf}
        \caption{PSNR across different sampling probabilities}
        \label{fig:img2}
    \end{subfigure}

    \caption{Monte-carlo converge results}
    \label{fig:both_images}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.65\linewidth]{res/window_size_mse.pdf}
    \caption{NLM vs MCNLM mse for different window sizes}
    \label{fig1}
\end{figure}


