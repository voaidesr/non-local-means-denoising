\section{Monte Carlo Non-Local Means}

In this section, we discuss the full implementation of the algorithm, the correctness of the approach and some other optimizations that can be added to the method.

\subsection{Sampling Patches}

There are two ways to extract reference patches. The first one is based on picking them from the original noisy, image, which is called \textit{internal denoising}. The second method is based on having the patches taken from a large database of other images, which is called \textit{external denoising}. The paper focuses on internal denoising.

The Monte Carlo sampling is done on the set $\mathcal{X} = \{x_1, \dots, x_n \}$, considering each reference patch independent. The process is determined by a sequence of random variables $\{I_j\}_{j=1}^n$, where $I_j \sim \operatorname{Bernoulli}(p_j)$. The weight will be sampled only if $I_j = 1$. The vector of these probabilities, $\mathbf{p} := [p_1, \dots, p_n]^T$, is called the \textit{sampling pattern} of the algorithm \cite{mcnlm}.

The main parameter of this algorithm is $\xi$, which is the expected value of the random variable which models the ratio between the number of the samples taken and the number of references:
\begin{equation}
    S_n = \frac{1}{n} \sum_{j = 1}^n I_j
\end{equation}
\noindent which has:
\begin{equation}
    \xi \overset{\text{def}}{=} \mathbb{E}[S_n] = \frac{1}{n} \sum_{j = 1}^{n} \mathbb{E}[I_j] = \frac{1}{n}\sum_{j = 1}^{n} p_j
\end{equation}

So, an important requirement is that:
\begin{equation}
    \sum_{j=1}^{n} = n \xi
\end{equation}

$S_n$ is called the \textit{empirical sampling ratio} and $\xi$ the average sampling ratio \cite{mcnlm}.

\subsection{Algorithm}
\label{sec:algo}
Given a set of references $\mathcal{X}$ and the sampling ration $\xi$, we can define the sampling pattern $\mathbf{p}$ in order for it to respect the condition:
\begin{equation}
    \sum_{j = 1}^{n} p_j  = n \xi
\end{equation}.

We can approximate the numerator and the denominator in \ref{eq:nlm} using two random variables:
\begin{equation}
    A = \frac{1}{n} \sum_{j = 1}^{n} x_j w_j \frac{I_j}{p_j} \quad \text{  and  } \quad
    B = \frac{1}{n} \sum_{j = 1}^{n} w_j \frac{I_j}{p_j}
\end{equation}

To show why we scale by $\frac{1}{p_j}$, we calculate the expected value of the estimators. By linearity of expectation:
\begin{equation}
    \mathbb{E}[A] = \frac{1}{n} \sum_{j=1}^{n} x_j w_j \frac{\mathbb{E}[I_j]}{p_j}
    = \frac{1}{n} \sum_{j=1}^{n} x_j w_j \frac{p_j}{p_j} =  \frac{1}{n} \sum_{j=1}^{n} x_j w_j
\end{equation}
\noindent and
\begin{equation}
    \mathbb{E}[B] = \frac{1}{n} \sum_{j = 1}^{n} w_j \frac{\mathbb{E}[I_j]}{p_j} = \frac{1}{n} \sum_{j = 1}^n w_j
\end{equation}

So, A and B are \textit{unbiased estimators} of the true numerator and denominator.

In the end, we will aproximate the result $z$ by another random variable:
\begin{equation}
    Z = \frac{A}{B} = \frac{\sum_{j=1}^{n} x_j w_j \frac{I_j}{p_j}}{\sum_{j = 1}^{n} w_j\frac{I_j}{p_j}}
\end{equation}

However,
\begin{equation}
    \mathbb{E}[Z] = \mathbb{E}\left[\frac{A}{B}\right] \neq \frac{\mathbb{E}[A]}{\mathbb{E}[B]} = z
\end{equation}

% todo - implement section label sec:ineq when I atually implement the inequality
\noindent so, $Z$ is a \textit{biased estimator} of $z$. However, section \ref{sec:ineq} proves that as $n \to \infty$, the deviation $|Z - z|$ drops exponentially. This shows that, for a larger number of patches, the more efficient MCNLM gives results comparable to that of the simple NLM.

\FloatBarrier
\begin{algorithm}[H]
\caption{Monte Carlo NLM Denoising}
\label{alg:mcnlm}
\begin{algorithmic}[1]
\Require Noisy patch $\mathbf{y}$, reference set $\mathcal{X}$, sampling pattern $\mathbf{p}$
\Ensure Denoised pixel $z$
\For{$j$ in $1 \dots n$}
    \State Generate random variable $I_j \sim \operatorname{Bernoulli}(p_j)$
    \If{$I_j = 1$}
        \State Compute $w_j$
    \EndIf
\EndFor
\State Compute $A = \frac{1}{n} \sum_{j = 1}^{n} w_j x_j \frac{I_j}{p_j}$
\State Compute $B = \frac{1}{n} \sum_{j = 1}^{n} w_j  \frac{I_j}{p_j}$
\State \Return $Z = \frac{A}{B}$
\end{algorithmic}
\end{algorithm}


\subsection{Spatial Sampling}

We must also consider that locality matters in images. In a picture of a starry night, for example, a bright star might look like noise if compared to the rest of the dark sky. We can integrate this insight with the ingenious structural handling of MCNLM by incorporating a spatial distance parameter into the weighting function:
\begin{equation}
    w_j = w_j^s \cdot w_j^r
\end{equation}
\noindent where $w_j^r$ is the weight mentioned at \ref{eq:wr} and $w_j^s$ is a spatial weight:
\begin{equation}
w_j^s = e^{-(d_2^j)^2/(2h_s^2)} \cdot \mathbb{I}\{d^j_{\infty} \leq \rho\}
\end{equation}

\noindent where $d_j^2$ is the Euclidean distance between the noisy patch and the reference patch we compare to and $d_j^{\infty}$ is the Chebyshev distance. The  indicator function $\mathbb{I}$ is used to determine a spatial search window of width $\rho$.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{res/mc_matches_1.pdf}
        \caption{Weight values in search window}
        \label{fig:img1}
    \end{subfigure}
    \hfill % Adds flexible space between images
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{res/mc_matches_2.pdf}
        \caption{Weigths for sampled center pixels}
        \label{fig:img2}
    \end{subfigure}

    \caption{Main caption describing both images}
    \label{fig:both_images}
\end{figure}

Figure \ref{fig:both_images} visualizes the weight function within a search window centered on a noisy pixel, where brighter values indicate higher weight magnitudes. This illustrates how the method preserves structure: pixels with similar patches exert a stronger influence on the weighted average, ensuring that structural details are retained in the denoised output.

\subsection{Results}

\FloatBarrier
\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{res/mcnlm2.pdf}
    \caption{MCNLM with $\xi \in \{0.3,  0.5, 0.8\}$}
    \label{fig1}
\end{figure}
% aici vreau sa avem results cu diferite valori xi, comparate

In the figures \ref{fig1}, \ref{fig2} we can see how the MCNLM algorithm behaves on an 1024x1024 image ($\approx 10^6$ pixels), with Gaussian noise with $\sigma = 17 / 255$ (taken into account that we normalize our pixels to have values in the interval $[0,1]$)/ and zero mean. The comparison is done on 5x5 patches, and on a 20x20 search window arround the noisy pixel. The sampling pattern is chosen as uniform, with $\mathbf{p} = \{\xi, \cdots, \xi\}$. It is obvious that an increase in the sampling ratio only provides a marginally better result.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{res/mcnlm3.pdf}
    \caption{MCNLM with $\xi \in \{0.3,  0.5, 0.8\}$}
    \label{fig2}
\end{figure}

\FloatBarrier