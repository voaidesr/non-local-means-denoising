\section{Monte Carlo Non-Local Means}

In this section, we discuss the full implementation of the algorithm, the correctness of the approach and some other optimizations that can be added to the method.

\subsection{Sampling Patches}

There are two ways to extract reference patches. The first one is based on picking them from the original noisy, image, which is called \textit{internal denoising}. The second method is based on having the patches taken from a large database of other images, which is called \textit{external denoising}. The paper focuses on internal denoising.

The Monte Carlo sampling is done on the set $\mathcal{X} = \{x_1, \dots, x_n \}$, considering each reference patch independent. The process is determined by a sequence of random variables $\{I_j\}_{j=1}^n$, where $I_j \sim \operatorname{Bernoulli}(p_j)$. The weight will be sampled only if $I_j = 1$. The vector of these probabilities, $\mathbf{p} := [p_1, \dots, p_n]^T$, is called the \textit{sampling pattern} of the algorithm \cite{mcnlm}.

The main parameter of this algorithm is $\xi$, which is the expected value of the random variable which models the ratio between the number of the samples taken and the number of references:
\begin{equation}
    S_n = \frac{1}{n} \sum_{j = 1}^n I_j
\end{equation}
\noindent which has:
\begin{equation}
    \xi \overset{\text{def}}{=} \mathbb{E}[S_n] = \frac{1}{n} \sum_{j = 1}^{n} \mathbb{E}[I_j] = \frac{1}{n}\sum_{j = 1}^{n} p_j
\end{equation}

So, an important requirement is that:
\begin{equation}
    \sum_{j=1}^{n} = n \xi
\end{equation}

$S_n$ is called the \textit{empirical sampling ratio} and $\xi$ the average sampling ratio \cite{mcnlm}.

\subsection{Algorithm}
\label{sec:algo}
Given a set of references $\mathcal{X}$ and the sampling ration $\xi$, we can define the sampling pattern $\mathbf{p}$ in order for it to respect the condition:
\begin{equation}
    \sum_{j = 1}^{n} p_j  = n \xi
\end{equation}.

We can approximate the numerator and the denominator in \ref{eq:nlm} using two random variables:
\begin{equation}
    A = \frac{1}{n} \sum_{j = 1}^{n} x_j w_j \frac{I_j}{p_j} \quad \text{  and  } \quad
    B = \frac{1}{n} \sum_{j = 1}^{n} w_j \frac{I_j}{p_j}
\end{equation}

To show why we scale by $\frac{1}{p_j}$, we calculate the expected value of the estimators. By linearity of expectation:
\begin{equation}
    \mathbb{E}[A] = \frac{1}{n} \sum_{j=1}^{n} x_j w_j \frac{\mathbb{E}[I_j]}{p_j}
    = \frac{1}{n} \sum_{j=1}^{n} x_j w_j \frac{p_j}{p_j} =  \frac{1}{n} \sum_{j=1}^{n} x_j w_j
\end{equation}
\noindent and
\begin{equation}
    \mathbb{E}[B] = \frac{1}{n} \sum_{j = 1}^{n} w_j \frac{\mathbb{E}[I_j]}{p_j} = \frac{1}{n} \sum_{j = 1}^n w_j
\end{equation}

So, A and B are \textit{unbiased estimators} of the true numerator and denominator.

In the end, we will aproximate the result $z$ by another random variable:
\begin{equation}
    Z = \frac{A}{B} = \frac{\sum_{j=1}^{n} x_j w_j \frac{I_j}{p_j}}{\sum_{j = 1}^{n} w_j\frac{I_j}{p_j}}
\end{equation}

However,
\begin{equation}
    \mathbb{E}[Z] = \mathbb{E}\left[\frac{A}{B}\right] \neq \frac{\mathbb{E}[A]}{\mathbb{E}[B]} = z
\end{equation}

% todo - implement section label sec:ineq when I atually implement the inequality
\noindent so, $Z$ is a \textit{biased estimator} of $z$. However, section \ref{sec:ineq} proves that as $n \to \infty$, the deviation $|Z - z|$ drops exponentially. This shows that, for a larger number of patches, the more efficient MCNLM gives results comparable to that of the simple NLM.

\FloatBarrier
\begin{algorithm}[H]
\caption{Monte Carlo NLM Denoising}
\label{alg:mcnlm}
\begin{algorithmic}[1]
\Require Noisy patch $\mathbf{y}$, reference set $\mathcal{X}$, sampling pattern $\mathbf{p}$
\Ensure Denoised pixel $z$
\For{$j$ in $1 \dots n$}
    \State Generate random variable $I_j \sim \operatorname{Bernoulli}(p_j)$
    \If{$I_j = 1$}
        \State Compute $w_j$
    \EndIf
\EndFor
\State Compute $A = \frac{1}{n} \sum_{j = 1}^{n} w_j x_j \frac{I_j}{p_j}$
\State Compute $B = \frac{1}{n} \sum_{j = 1}^{n} w_j  \frac{I_j}{p_j}$
\State \Return $Z = \frac{A}{B}$
\end{algorithmic}
\end{algorithm}


\subsection{Spatial Sampling}

We must also consider that locality matters in images. In a picture of a starry night, for example, a bright star might look like noise if compared to the rest of the dark sky. We can integrate this insight with the ingenious structural handling of MCNLM by incorporating a spatial distance parameter into the weighting function:
\begin{equation}
    w_j = w_j^s \cdot w_j^r
\end{equation}
\noindent where $w_j^r$ is the weight mentioned at \ref{eq:wr} and $w_j^s$ is a spatial weight:
\begin{equation}
w_j^s = e^{-(d_2^j)^2/(2h_s^2)} \cdot \mathbb{I}\{d^j_{\infty} \leq \rho\}
\end{equation}

\noindent where $d_j^2$ is the Euclidean distance between the noisy patch and the reference patch we compare to and $d_j^{\infty}$ is the Chebyshev distance. The  indicator function $\mathbb{I}$ is used to determine a spatial search window of width $\rho$.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{res/mc_matches_1.pdf}
        \caption{Weight values in search window}
        \label{fig:img1}
    \end{subfigure}
    \hfill % Adds flexible space between images
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{res/mc_matches_2.pdf}
        \caption{Weigths for sampled center pixels}
        \label{fig:img2}
    \end{subfigure}

    \caption{Main caption describing both images}
    \label{fig:both_images}
\end{figure}

Figure \ref{fig:both_images} visualizes the weight function within a search window centered on a noisy pixel, where brighter values indicate higher weight magnitudes. This illustrates how the method preserves structure: pixels with similar patches exert a stronger influence on the weighted average, ensuring that structural details are retained in the denoised output.

\subsection{Results}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{res/mcnlm2.pdf}
    \caption{MCNLM with $\xi \in \{0.3,  0.5, 0.8\}$}
    \label{fig1}
\end{figure}
% aici vreau sa avem results cu diferite valori xi, comparate

In the figures \ref{fig1}, \ref{fig2} we can see how the MCNLM algorithm behaves on an 1024x1024 image ($\approx 10^6$ pixels), with Gaussian noise with $\sigma = 17 / 255$ (taken into account that we normalize our pixels to have values in the interval $[0,1]$)/ and zero mean. The comparison is done on 5x5 patches, and on a 20x20 search window arround the noisy pixel. The sampling pattern is chosen as uniform, with $\mathbf{p} = \{\xi, \cdots, \xi\}$. It is obvious that an increase in the sampling ratio only provides a marginally better result.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{res/mcnlm3.pdf}
    \caption{MCNLM with $\xi \in \{0.3,  0.5, 0.8\}$}
    \label{fig2}
\end{figure}


\section{Stochastic Approximation of Non-Local Means}
We yield the question whether the Monte-Carlo variant of our algorithm is indeeed reliable and aproaches the full Non-Local Means solution. For this we need to analyze the approximation error $|Z - z|$.

To understand our approximation, we will study the empirical sampling ratio $S_{n}$. The law of large numbers states that the empirical mean $S_{n}$ of a large number of indepent random variable is close to the true mean.

\subsection{Probability Bounds}
For any error bound $\varepsilon > 0$ and any sampling pattern $p$ with $0 < p_{j} <= 1$ and $\sum_{j=1}^{n}p_{j} = \varepsilon$ we want to study the probability: 
\begin{equation}
    \mathbb{P}(|S_{n} - \mathbb{E}[S_{n}]| > \varepsilon)
\end{equation}

From the Cebyshev inequality we know that:
\begin{equation}
    \mathbb{P}(|S_{n} - \mathbb{E}[S_{n}]| > \varepsilon) < \frac{Var[I_{1}]}{n\varepsilon^{2}}, \forall \varepsilon > 0
\end{equation}

This bound provided by the Cebyshev inequality above is too loose, only providing a linear descent of the error probability as $n \to \infty$. 

We can use the Bernstein inequality to provide a much tighter exponential bound. The Bernstein inequality states that for $X_{1}, \dots X_{n}$, a sequence of independent bounded random variables ($l_{j} <= X_{j} <= u_{j}$, where $u_{j}$ and $l_{j}$ are constants), the following is true:
\begin{equation}
    \mathbb{P}(|S_{n} - \mathbb{E}[S_{n}]| > \varepsilon) \le exp \left (-\frac{n\varepsilon^2}{2(\frac{1}{n}\sum_{j=1}^{n}Var[X_{j}] + M\epsilon/3)} \right )
\end{equation}

where $S_{n} = \frac{1}{n}\sum_{j=1}^{n}X_{j}$ and $M = max_{1 \le j \le n}\frac{u_{j} - l_{j}}{2}$

In our situation, $X_{j} = I_{j}$, $M=1$, $\mathbb{E}[S_{n}] = \xi$, so:
$$
\frac{1}{n}\sum_{j=1}^{n}Var[X_{j}] = \frac{1}{n}\sum_{j=1}^{n}p_{j}(1-p_{j}) = \xi(1-\xi).
$$

By substituing this result in the equation above we get an exponential bound on the error probability.


\subsection{General Error Probability Bound}
To rigorously quantify the reliability of the Monte Carlo approximation, we refer to the concentration bound derived in the literature. For a sample size $n$ and an error tolerance $\varepsilon > 0$, the probability that the Monte Carlo estimate $Z(\mathbf{p})$ deviates from the exact NLM value $z$ is bounded by exponential decay terms:

\begin{equation}
    \begin{split}
    \mathbb{P}(|Z(\boldsymbol{p}) - z| > \varepsilon ) \leq & \exp \{-n\xi\} \\
    & + \exp \left\{ \frac{-n(\mu_B \varepsilon)^2}{ 2 \left( \frac{1}{n} \sum_{j=1}^n \alpha_j^2 \left( \frac{1-p_j}{p_j} \right) + (\mu_B \varepsilon)M_\alpha/6 \right) } \right\} \\
    & + \exp \left\{ \frac{-n(\mu_B \varepsilon)^2}{ 2 \left( \frac{1}{n} \sum_{j=1}^n \beta_j^2 \left( \frac{1-p_j}{p_j} \right) + (\mu_B \varepsilon)M_\beta/6 \right) } \right\},
    \end{split}
\end{equation}

where:
\begin{itemize}
    \item $\xi$ represents the sampling density (e.g., $p_j$ in uniform sampling).
    \item $\mu_B$ is the average similarity weight (expected value of the denominator).
    \item $K$ is a constant bound dependent on the variance of the weights in the search window.
\end{itemize}


\textbf{Example: }
To validate this bound, we consider a concrete example of a one-dimensional signal with length $n = 10^4$, corrupted by Gaussian noise with standard deviation $\sigma = 5/255$.

We apply Monte Carlo NLM to denoise a pixel using a \textbf{uniform sampling pattern} where only $5\%$ of the pixels are sampled ($p_j = \xi = 0.05$ for all $j$). We set an error tolerance of $\varepsilon = 0.01$.

Using the derived constants:
\begin{itemize}
    \item Average similarity weight: $\mu_B = 0.3015$
    \item Variance term 1: $\frac{1}{n} \sum \alpha_j^2 \approx 1.335 \times 10^{-4}$
    \item Variance term 2: $\frac{1}{n} \sum \beta_j^2 \approx 1.452 \times 10^{-4}$
\end{itemize}

And substituting these values into the probability bound yields:
\begin{equation}
    \mathbb{P}(|Z(\mathbf{p}) - z| > 0.01 ) \leq 6.516 \times 10^{-6}
\end{equation}

\vspace*{1cm}

\textbf{Conclusion:} This result demonstrates that even when using only \textbf{5\% of the samples}, the Monte Carlo estimate stays within $1\%$ of the true NLM result with overwhelming probability ($1 - 6.5 \times 10^{-6}$). This theoretical calculation validates the rapid MSE convergence observed in our empirical charts.


\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{res/convergence1_mse.pdf}
        \caption{MSE across different sampling probabilities}
        \label{fig:img1}
    \end{subfigure}
    \hfill % Adds flexible space between images
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{res/convergence1_psnr.pdf}
        \caption{PSNR across different sampling probabilities}
        \label{fig:img2}
    \end{subfigure}

    \caption{Monte-carlo converge results}
    \label{fig:both_images}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.65\linewidth]{res/window_size_mse.pdf}
    \caption{NLM vs MCNLM mse for different window sizes}
    \label{fig1}
\end{figure}


